\pdfoutput=1

\documentclass[11pt]{article}

\usepackage{wiki-savvy-rag}
\usepackage{times}
\usepackage{latexsym}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{inconsolata}
\usepackage{natbib} % this is probably into you nips_2018 format file
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{subcaption}


\newcommand{\sideBySideImages}[4]{%
    \begin{figure}[ht]
        \centering
        \begin{subfigure}{#3\linewidth}
            \centering
            \includegraphics[width=\linewidth]{#1}
        \end{subfigure}%
        \hfill%
        \begin{subfigure}{#3\linewidth}
            \centering
            \includegraphics[width=\linewidth]{#2}
        \end{subfigure}
        \caption{Side by side images with width parameter}
        \label{#4}
    \end{figure}
}

% TODOS:
% - Add the MMLU results and figures
% - add training figures
% - In the abstract, briefly mention our findings once we have some
% - embeddings section: first one to go


\title{Wikipedia-Savvy-RAG: Wikipedia-backed RAG system expert about STEM subjects}  

\author{Dario Filatrella \and Michele Palma \and Mattia Scardecchia \and Federico Zarantonello}

\begin{document}
\maketitle
\begin{abstract}
We present Wikipedia-Savvy-RAG, a lightweight Retrieval-Augmented Generation (RAG) system for
STEM domain question answering. To improve performance in knowledge intensive tasks we filtered, cleaned, 
chunked and embedded the English Wikipedia and built a vector database of STEM-focused semantic embeddings. 
Then, we used QLORA to efficiently finetune a small pretrained language model 
(500M params) on a subset of the Yahoo Answers dataset, feeding the model with the most relevant passages from the 
vector database and keeping the query and passage embedders frozen. Finally, we used the MMLU dataset to 
benchmark our system and run a variety of ablation studies, using the pretrained llm without RAG as our baseline.
The system is lightweight and can be deployed on most modern personal computers. To demonstrate its capabilities,
we developed a Streamlit web application that allows users to interact with the model in real-time with a chat-like interface.
Experiments are fully reproducible and the code is available at \url{https://github.com/spring-projects-2024/wiki-savvy-rag}.

\end{abstract}

\section{Introduction and Related Works}

In recent years, the field of natural language processing has seen a surge in
the development of large language models (LLMs) that can generate human-like text. \cite{gpt3}
These models have been used in a variety of applications, such as chatbots,
question-answering systems, and text summarization, with remarkable performance.  \cite{nlp_applications}
However, LLMs often struggle with generating accurate and informative responses, especially in specialized
domains like science, technology, engineering, and mathematics (STEM).  \cite{hallucinations_llm}

A successful approach to address this limitation has been the combination of LLMs with retrieval
mechanisms from a knowledge base to generate more accurate and informative responses.
Along this line of research, started with \cite{rag_paper}, several variants and improvements have been proposed,
such as \cite{replug} \cite{radit}. These are extensively reviewed in \cite{rag_survey}.

In this paper, we present Wikipedia-Savvy-RAG, a lightweight RAG system fine-tuned for STEM question answering
in chat format. In building our system, we strived to keep the computational resources required for both
training and inference contained. Indeed, our system can be deployed on personal computers with a CUDA-enabled GPU
with at least 8GB of memory, through a Streamlit web application we developed. Furthermore, the finetuning can 
be replicated on a single consumer-grade GPU in a few hours. In doing so, the main research question that we aimed to
answer is whether and to what extent a small llm can benefit from retrieval in knowledge intensive tasks, and whether a
simple finetuning recipe under tight computational constraints can still improve performance. 

To build the vector database, we processed the English Wikipedia available on WikiMedia \cite{wikimedia} and obtained 
dense representations of the chunks using a lightweight text embedder \cite{baai}. We then built a vector database using
the Faiss library \cite{faiss} and SQLite \cite{sqlite} to index and retrieve embeddings and passages, respectively. To keep resources low,
we employed a small pretrained LLM with 500M parameters \cite{qwen}, and we used QLORA \cite{qlora} to finetune it efficiently, 
keeping the embedder frozen, on a subset of the Yahoo Answers dataset \cite{yahoo_answers}.
In order to benchmark our system and conduct a variety of ablation studies, we used the MMLU dataset \cite{mmlu}.

This report will outline the steps we took to build our system, the motivations behind our choices, and the results of our evaluation.
We will also discuss potential improvements and future work.

\section{Knowledge Base}

Since we decided to focus on STEM subjects, we needed a knowledge base that was rich in scientific and technical information.
We downloaded the English Wikipedia dump, filtered the articles related to STEM subjects, and cleaned the text.

\subsection{Filtering and Cleaning}

We downloaded the dump dated 20 December 2023 (100GB) from WikiMedia \cite{wikimedia}. Wikipedia articles have category tags to help classify them.
Categories are organized in a directed acyclic graph (DAG), but not a tree \cite{wikipedia_category_tree}. To keep STEM articles, we marked only the categories that are at
distance at most k from a set of hand-picked root categories in the category DAG, and we kept articles belonging to marked categories.
Then, we used regular expressions and many python scripts to remove (almost) everything that wasn't plain text, such as XML tags, tables, and references. After filtering, the XML
dump was reduced to 22GB. After cleaning, it went down to 8GB.

Then we proceeded in splitting the articles into the different sections to obtain the chunks 
that will be used by the RAG model. 
In the end we obtained a total of 13 million chunks, with an average length of about 230 words.
We stored all of these, together with their titles, in an SQLite database.

\subsection{Embeddings}

To prepare the raw text for semantic embedding, we used spacy to chunk the articles into 13 million passages with an average
length of 230 words, and stored them in an SQLite database. Then, we tokenized and embedded them using the open-source BAAI/bge-small-en-v1.5 model 
\textit{link}. This model is the smaller version of the BAAI General Embeedding family v1.5. 
To get the dense embeddings for sentences, it uses the last hidden state of the [CLS] token, and it outputs 
384-dimensional vectors.

We chose this model for its balance between performance and computational cost.
In fact, the embedder is significantly smaller than its peers, being a BERT-like model with 33.4M 
parameters and only 0.12GB of memory usage, but achieves a 51.68 score on the MTEB Retrieval task.

\subsection{Vector Database}

We used the Faiss library to build a vector database of the embeddings for efficient retrieval, using cosine similarity to compare vectors.
Faiss is a library for efficient similarity search and clustering of dense vectors that offers different indexing methods.
We mostly investigated three index variants: Flat uses simple brute-force search, IVF uses centroid-based clustering to 
implement a hierarchical search that reduces the number of comparisons, and HNSW creates a multi-layered graph structure 
where each layer is a simplified, navigable small world network. For further details, see the excellent Faiss documentation \cite{faiss}.

Faiss also offers different quantization techniques to reduce the memory footprint of the embeddings. We focused
on two quantization techniques: scalar quantization and product quantization. 
Scalar quantization quantizes each dimension of the vector independently in a linear
range, while the product quantization splits the vector into subvectors and quantizes each
subvector independently.

We benchmarked extensively different combinations of indexes and quantization techniques to find the best configuration
for our system. We measured recall, query time, and memory usage to find a good trade-off. See the Benchmark section for more details.
We eventually chose the Flat index with 128 subvectors product quantization.

\section{Retrieval-Augmented Generation}

\subsection{Large Language Model}

As llm, we chose the pretrained Qwen/Qwen1.5-0.5B-Chat \cite{qwen_hf} model, the smallest model in the Qwen v1.5 family
with only 620M parameters. It was pretrained with a large amount of data, and post-trained with both supervised finetuning 
and direct preference optimization to allign it with human preferences. It has a context length of 32K tokens.

\subsection{Generation}

To generate text through RAG, we need to pass retrieved passages to the model in order to produce logits
for the next token, and then use a decoding strategy to generate the final answer.
We implemented two methods to produce logits. The first one, which we refer to as 'naive', concatenates all the retrieved passages
and the query, and feeds the result to the model. The second one, following \cite{replug}, feeds the query and each individual passage
separately to the model, and then combines the predicted probabilities from each passage with weights proportional to the similarity
to get probabilities.
As for the decoding strategy, we implemented greedy decoding, top-k sampling, and top-p sampling. \cite{decoding}

\section{Finetuning}

The llm was pretrained alligned, but it was never tought to consider retrieved passages to compose its answer to a user query.
To address this, we finetuned our system on a question answering task, using a simple recipe.

\subsection{Data}

As training data, we downloaded the Yahoo Answers dataset and used its metadata to filter it and keep only STEM questions. The result is around 23k
examples. As validation data, we used the validation split of the MMLU dataset \cite{mmlu_hf}. During training, for a given question, we retrieved the single most similar passage from
our knowledge base and formatted the input to meet the expectations of Qwen, including the retrieved passage as context.

\subsection{Training}

We froze the query and passage embedders and finetuned the llm only. We used the QLORA technique \cite{qlora} to drastically reduce the number of training
parameters and the memory requirements while minimally impacting performance. In essence, this consists in freezing and quantizing the pretrained weights and adding low
rank adapters to each layer, as in \cite{lora}. Then, we backpropagate through the 4-bit quantized frozen weights to train the adapters.
As a training criterion, we use cross entropy loss between predicted logits and the ground truth answer. This allows exploiting the inherent
parallelism of the transformer architecture, since thanks to the causal mask we obtain an error signal for each ground truth token in parallel at every step.
Details about our training setup can be found in out GitHub repository.

\section{Demo}

To demonstrate the capabilities of our system, we developed a Streamlit web application
that allows users to interact with the model in real-time with a chat-like interface. 
The application lets users ask questions about STEM subjects, and the model will provide answers 
based on the Wikipedia passages it retrieves from its knowledge base. 
The user can decide the number of passages to retrieve, the decoding strategy (greedy, top-k, top-p), 
the generation type (naive or REPLUG) and other configuration parameters.
Refer to our GitHub repository for more information on the application.

\subsection{Arxiv}

\textit{???????????????????????????}

\section{Benchmark}

To evaluate the performance of our system, we conducted several experiments.
We compared the performance of different Faiss indexes, the accuracy of the system with and without RAG,
and the accuracy of the system before and after fine-tuning.

\subsection{Faiss indexes}

The task was to retrieve the k most similar passages to a
given query coming from the MMLU dataset.
The parameters we considered were the recall, the query time, 
and the memory usage. 
The recall was computed as the intersection measure between the retrieved 
passages and the ground truth. 
The variations we considered were the index type (Flat, IVF, HNSW), the number 
of clusters for the IVF index, the quantization technique (product, scalar), and
the amount of quantization bits.

\subsection{MMLU}

The Measuring Massive Multitask Language Understanding (MMLU) is a collection of 
multiple-choice questions that spans a wide range of topics.
We hand-picked the subcategories that according to us were part of the STEM domain and
obtain a total of about 3000 questions.
We benchmarked the system considering different variations of it. First we compared the 
performances changing the number of retrieved passages; then we changed the number of 
examples in the prompt; finally we compared the different inference strategies (naive and REPLUG).
To assess the performances throughout training steps, we compared the performances of different 
model checkpoints.


\section{Results}

\subsection{Faiss indexes}

We measured the recall of the indexes on k=1, 10, 50 and 100. The query time was 
the time it took to retrieve the 100 most similar passages for each one of the first 300
queries in the MMLU dataset with STEM subject. 
For the memory usage, we considered the size of the index in disk, which acts as a lower
bound for the memory usage.
The most interesting results are shown in Table \ref{tab:faiss-index}, where the indexes 
are the following:

% ordered list with numbers
\begin{enumerate}
    \item Flat index with 4 bits scalar quantization.
    \item Flat index with 8 bits scalar quantization.
    \item Flat index with 128 subvectors product quantization. 
    \item Index with 256 subvectors product quantization, IVF with 1000 centroids
    and HNSW32 as coarse quantizer.
    \item HNSW index with 16 bits scalar quantization.
\end{enumerate}

\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
Index & Recall & Query Time & Memory \\
\hline
1 & 0.832 & 03:53.59 & 2,609 \\
2 & 0.9873 & 03:29.50 & 5,219 \\
3 & 0.781 & 01:55.76 & 1,740 \\
4 & 0.4643 & 00:00.03 & 1,856 \\
5 & 0.661 & 00:00.11 & 4,570 \\
\hline
\end{tabular}
\caption{Faiss index comparison.
Here Query Time is evaluated on 300 MMLU queries and Memory is expressed in MB.
}
\
\label{tab:faiss-index}
\end{table}

As it is expected, Flat indexes are the ones that perform the best in terms of recall.
Here what really matters is the trade-off between compression level and accuracy.
The query time significantly decreseas with the use of IVF and HNSW indexes, but the recall
is also affected.

In the end, since the query time was fine for our purposes, we chose the Flat index with 128
subvectors product quantization, which had the lowest memory usage and a good recall.

\subsection{Training}

We trained our model for slightly over 2 epochs, measuring the average cross-entropy on both the training and validation sets. We used weights
and biases to monitor the training process through a variety of metrics and statistics, including parameters and gradients histograms.
Due to limited memory, we had to resort to gradient accumulation to simulate a larger batch size than 1 (we used 2). The training loss, 
with enough smoothing, decreased steadily, as expected. As for the validation loss, after an initial sharp decrease, it started increasing
again with an oscillatory pattern. This is probably due in part to the use of different datasets for training and validation, which we did
to measure generalization more reliably. The sharp drop in validation loss right at the beginning of training could be due to the 4 bit quantization,
as in the first few steps the model is still learning to correct the quantization errors. Training curves are shown in figure \ref{fig:training_curves}.

\sideBySideImages{train-loss.png}{val-loss.png}{0.48}{fig:training_curves}

\subsection{MMLU}

\section{Conclusion}
We developed, fine-tuned, and evaluated a Retrieval-Augmented Generation (RAG) 
system for STEM domain discussions. Our application features a chat interface 
that enhances responses using retrieved Wikipedia passages. We processed the 
English Wikipedia to build a vector database of STEM-focused semantic 
embeddings. 
We assessed the system using the MMLU 
dataset, fine-tuning the LLM for improved performance. Evaluations compared 
performance with and without RAG, before and after fine-tuning.
Results show significant accuracy improvements, highlighting RAG's potential 
in specialized domains like STEM.

One note is that we tried to implement some of the components from scratch
to have a better understanding of the system and to have more control over the
implementation.
This choice wasn't always the most efficient, but it allowed us to learn a lot
about the inner workings of retrieval augmented systems and processing of natural 
language in general. 

We think that this work demonstrates the potential of RAG systems in specialized 
domains like STEM and provived us with valuable insights about the best practices 
to follow when building such systems.

\newpage

\bibliography{wiki-savvy-rag}
\bibliographystyle{acl_natbib}
\nocite{*}

\appendix

\section{Appendix}
\label{sec:appendix}

\subsection{Other ablations on MMLU}

In \ref{fig:mmlu_passages} we show the performances of the system with different 
number of retrieved passages. The considered values are 0 (no retrieval), 1, 3 and 5.

In \ref{fig:mmlu_shots} we compare the different number of examples (shots) given to
the model in the prompt. The considered values are 0, 1, 3 and 5.

In \ref{fig:mmlu_strategies} we compare the different inference strategies (naive and REPLUG).

\subsection{Checkpoints comparison on MMLU}

In \ref{fig:checkpoints} we compare the performances of different model checkpoints on the MMLU dataset.

\end{document}
