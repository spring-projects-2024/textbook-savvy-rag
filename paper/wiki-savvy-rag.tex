\pdfoutput=1

\documentclass[11pt]{article}

\usepackage{wiki-savvy-rag}
\usepackage{times}
\usepackage{latexsym}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{inconsolata}
\usepackage{natbib} % this is probably into you nips_2018 format file

% TODOS:
% - In the abstract, briefly mention our findings once we have some
% - Add the training section
% - Add the MMLU results
% - Add the training results
% - Add bibliography


\title{Wikipedia-Savvy-RAG: Wikipedia-backed RAG system expert about STEM subjects}  

\author{Dario Filatrella \and Michele Palma \and Mattia Scardecchia \and Federico Zarantonello}

\begin{document}
\maketitle
\begin{abstract}
We present Wikipedia-Savvy-RAG, a lightweight Retrieval-Augmented Generation (RAG) system for
STEM domain question answering. To improve performance in knowledge intensive tasks we filtered, cleaned, 
chunked and embedded the English Wikipedia and built a vector database of STEM-focused semantic embeddings. 
Then, we used QLORA to efficiently finetune a small pretrained language model 
(500M params) on a subset of the Yahoo Answers dataset, feeding the model with the most relevant passages from the 
vector database and keeping the query and passage embedders frozen. Finally, we used the MMLU dataset to 
benchmark our system and run a variety of ablation studies, using the pretrained llm without RAG as our baseline.
The system is lightweight and can be deployed on most modern personal computers. To demonstrate its capabilities,
we developed a Streamlit web application that allows users to interact with the model in real-time with a chat-like interface.

\end{abstract}

\section{Introduction and Related Works}

In recent years, the field of natural language processing has seen a surge in
the development of large language models (LLMs) that can generate human-like text. \cite{gpt3}
These models have been used in a variety of applications, such as chatbots,
question-answering systems, and text summarization, with remarkable performance.  \cite{nlp_applications}
However, LLMs often struggle with generating accurate and informative responses, especially in specialized
domains like science, technology, engineering, and mathematics (STEM).  \cite{hallucinations_llm}

A successful approach to address this limitation has been the combination of LLMs with retrieval
mechanisms from a knowledge base to generate more accurate and informative responses.
Along this line of research, started with \cite{rag_paper}, several variants and improvements have been proposed,
such as \cite{replug} \cite{radit}. These are extensively reviewed in \cite{rag_survey}.

In this paper, we present Wikipedia-Savvy-RAG, a lightweight RAG system fine-tuned for STEM question answering
in chat format. In building our system, we strived to keep the computational resources required for both
training and inference contained. Indeed, our system can be deployed on personal computers with a CUDA-enabled GPU
with at least 8GB of memory, through a Streamlit web application we developed. Furthermore, the finetuning can 
be replicated on a single consumer-grade GPU in a few hours. In doing so, the main research question that we aimed to
answer is whether and to what extent a small llm can benefit from retrieval in knowledge intensive tasks, and whether a
simple finetuning recipe under tight computational constraints can still improve performance. 

To build the vector database, we processed the English Wikipedia available on WikiMedia \cite{wikimedia} and obtained 
dense representations of the chunks using a lightweight text embedder \cite{baai}. We then built a vector database using
the Faiss library \cite{faiss} and SQLite \cite{sqlite} to index and retrieve embeddings and passages, respectively. To keep resources low,
we employed a small pretrained LLM with 500M parameters \cite{qwen}, and we used QLORA \cite{qlora} to finetune it efficiently, 
keeping the embedder frozen, on a subset of the Yahoo Answers dataset \cite{yahoo_answers}.
In order to benchmark our system and conduct a variety of ablation studies, we used the MMLU dataset \cite{mmlu}.

This report will outline the steps we took to build our system, the motivations behind our choices, and the results of our evaluation.
We will also discuss potential improvements and future work.

\section{System Overview}
Our system is composed of four main components.
An SQLite database stores the Wikipedia passages for faster reading.
An encoder-only model, commonly referred to as embedder, processes the queries and the 
Wikipedia passages to obtain a dense representation of them; in this way, similarity 
between the queries and the passages can be computed by simple dot products.
A vector database stores the embeddings for fast similarity search.
A RAG handler generates the answers using a LLM and the retrieved passages.

\section{Knowledge Base}

One of the main components of most of retrieval-based systems is the knowledge base.
In our case, we built a vector database of Wikipedia passages to be used by the RAG model.

\subsection{Cleaning Wikipedia}

The first step in building our knowledge base was to download and clean the English
Wikipedia dump. We downloaded the dump dated 20 December 2023 from WikiMedia \textit{cite} 
and extracted the categories that are related to STEM subjects. 
We then retrieved all the articles that belong to these categories and cleaned the text
by removing everything that wasn't plain text, such as XML tags, tables, and references.

Then we proceeded in splitting the articles into the different sections to obtain the chunks 
that will be used by the RAG model. 
In the end we obtained a total of 13 million chunks, with an average length of about 230 words.
We stored all of these, together with their titles, in an SQLite database.

\subsection{Embeddings}

Once we had the cleaned Wikipedia passages, we processed them with an embedder to obtain the dense 
representations.
We used the BAAI/bge-small-en-v1.5 model \textit{link}, available on the Hugging Face model hub.

The model is the smaller version of the BAAI General Embeedding family v1.5. 
To get the sentence embeddings, it uses the last hidden state of the [CLS] token, and it outputs 
384-dimensional vectors.

We chose this model for its balance between performance and computational cost.
In fact, the embedder is significantly smaller than its peers, being a BERT-like model with 33.4M 
parameters and only 0.12GB of memory usage, but achieves a 51.68 score on the MTEB Retrival task.

\subsection{Vector Database}

To handle efficiently the retrieval of the 13 million embedded chunks, we used the Faiss library. 
Faiss is a library for efficient similarity search and clustering of dense vectors. 
It offers different indexing methods. Flat uses simple brute-force search, 
IVF uses clustering to reduce the number of comparisons,
and HNSW creates a multi-layered graph structure where each layer is a simplified, 
navigable small world network. 

Faiss also offers different quantization techniques to reduce the memory usage of the embeddings, 
the two we considered are the scalar quantization and the product quantization. 
The scalar quantization quantizes each dimension of the vector independently in a linear
range, while the product quantization splits the vector into subvectors and quantizes each
subvector independently.

We compared the performance of different indexes and quantization techniques to choose the best
configuration for our system. See the Benchmark section for more details.
We eventually chose the Flat index with 128 subvectors product quantization.

\section{Retrieval-Augmented Generation}

The Retrieval-Augmented Generation (RAG) required to decide the Large Language Model (LLM) to use 
and the way to generate the answer given the top-k most similar passages retrieved from the knowledge base.
Similarity is calculated using the dot product between the query and the passages embeddings.

\subsection{Large Language Model}

The choice of the LLM was crucial for the performance of the system. Following the
same reasoning as for the embedder, we chose a model that was small but still powerful.
We chose the Qwen/Qwen1.5-0.5B-Chat \textit{link}, available on the Hugging Face model hub. 
This model is the smaller version of the Qwen family v1.5, and it has only 620M parameters, making
it one of the smallest models available on the hub.
It was pretrained with a large amount of data, and post-trained with both supervised finetuning 
and direct preference optimization. It has a context length of 32K tokens.

\subsection{Generation}

The generation process is the core of the RAG model. We implemented two methods to generate the
answers: the naive method and the REPLUG method.
The naive method is the simplest one: the model is feeded with the concatenation of all 
the retrieved passages and the query, and it generates the answer.
The REPLUG method is more complex: the model is given the query and each individual passage 
separately, generating an answer for each one. The final answer is then produced by 
combining the predicted probabilities from each passage.


\section{Finetuning}

\textit{Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed nec purus eget
nunc. Donec tincidunt, nunc in volutpat tempus, nulla justo varius lacus, vel
suscipit orci mi eget justo. Nulla sit amet magna in odio aliquam suscipit.}

\subsection{Data}

\textit{Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed nec purus eget
nunc. Donec tincidunt, nunc in volutpat tempus, nulla justo varius lacus, vel
suscipit orci mi eget justo. Nulla sit amet magna in odio aliquam suscipit.}

\subsection{Training}

\textit{Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed nec purus eget
nunc. Donec tincidunt, nunc in volutpat tempus, nulla justo varius lacus, vel
suscipit orci mi eget justo. Nulla sit amet magna in odio aliquam suscipit.}

\section{Demo}

To demonstrate the capabilities of our system, we developed a Streamlit web application
that allows users to interact with the model in real-time with a chat-like interface. 
The application lets users ask questions about STEM subjects, and the model will provide answers 
based on the Wikipedia passages it retrieves from its knowledge base. 
The user can decide the number of passages to retrieve, the decoding strategy (greedy, top-k, top-p), 
the generation type (naive or REPLUG) and other configuration parameters.
Refer to our GitHub repository for more information on the application.

\subsection{Arxiv}

\textit{???????????????????????????}

\section{Benchmark}

To evaluate the performance of our system, we conducted several experiments.
We compared the performance of different Faiss indexes, the accuracy of the system with and without RAG,
and the accuracy of the system before and after fine-tuning.

\subsection{Faiss indexes}

The task was to retrieve the k most similar passages to a
given query coming from the MMLU dataset.
The parameters we considered were the recall, the query time, 
and the memory usage. 
The recall was computed as the intersection measure between the retrieved 
passages and the ground truth. 
The variations we considered were the index type (Flat, IVF, HNSW), the number 
of clusters for the IVF index, the quantization technique (product, scalar), and
the amount of quantization bits.

\subsection{MMLU}

The Measuring Massive Multitask Language Understanding (MMLU) is a collection of 
multiple-choice questions that spans a wide range of topics.
For evaluating our system, we focused on the STEM subjects, which are the most
relevant for our application.
We hand-picked the subcategories that according to us were part of the STEM domain and
obtain a total of about 3000 questions.

\section{Results}

\subsection{Training}

\textit{Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed nec purus eget
nunc. Donec tincidunt, nunc in volutpat tempus, nulla justo varius lacus, vel
suscipit orci mi eget justo. Nulla sit amet magna in odio aliquam suscipit.}

\subsection{Faiss indexes}

We measured the recall of the indexes on k=1, 10, 50 and 100. The query time was 
the time it took to retrieve the 100 most similar passages for each one of the first 300
queries in the MMLU dataset with STEM subject. 
For the memory usage, we considered the size of the index in disk, which acts as a lower
bound for the memory usage.
The most interesting results are shown in Table \ref{tab:faiss-index}, where the indexes 
are the following:

% ordered list with numbers
\begin{enumerate}
    \item Flat index with 4 bits scalar quantization.
    \item Flat index with 8 bits scalar quantization.
    \item Flat index with 128 subvectors product quantization. 
    \item Index with 256 subvectors product quantization, IVF with 1000 centroids
    and HNSW32 as coarse quantizer.
    \item HNSW index with 16 bits scalar quantization.
\end{enumerate}

\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
Index & Recall & Query Time (300) & Memory (MB) \\
\hline
1 & 0.832 & 03:53.59 & 2,609 \\
2 & 0.9873 & 03:29.50 & 5,219 \\
3 & 0.781 & 01:55.76 & 1,740 \\
4 & 0.4643 & 00:00.03 & 1,856 \\
5 & 0.661 & 00:00.11 & 4,570 \\
\hline
\end{tabular}
\caption{Faiss index comparison}
\
\label{tab:faiss-index}
\end{table}

As it is expected, Flat indexes are the ones that perform the best in terms of recall.
Here what really matters is the trade-off between compression level and accuracy.
The query time significantly decreseas with the use of IVF and HNSW indexes, but the recall
is also affected.

In the end, since the query time was fine for our purposes, we chose the Flat index with 128
subvectors product quantization, which had the lowest memory usage and a good recall.


\subsection{MMLU}
\textit{Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed nec purus eget
nunc. Donec tincidunt, nunc in volutpat tempus, nulla justo varius lacus, vel
suscipit orci mi eget justo. Nulla sit amet magna in odio aliquam suscipit.}

\section{Conclusion}
We developed, fine-tuned, and evaluated a Retrieval-Augmented Generation (RAG) 
system for STEM domain discussions. Our application features a chat interface 
that enhances responses using retrieved Wikipedia passages. We processed the 
English Wikipedia to build a vector database of STEM-focused semantic 
embeddings. 
We assessed the system using the MMLU 
dataset, fine-tuning the LLM for improved performance. Evaluations compared 
performance with and without RAG, before and after fine-tuning.
Results show significant accuracy improvements, highlighting RAG's potential 
in specialized domains like STEM.

One note is that we tried to implement some of the components from scratch
to have a better understanding of the system and to have more control over the
implementation.
This choice wasn't always the most efficient, but it allowed us to learn a lot
about the inner workings of retrieval augmented systems and processing of natural 
language in general. 

We think that this work demonstrates the potential of RAG systems in specialized 
domains like STEM and provived us with valuable insights about the best practices 
to follow when building such systems.

\newpage

\bibliography{wiki-savvy-rag}
\bibliographystyle{acl_natbib}
\nocite{*}

\appendix

\section{Appendix}
\label{sec:appendix}

This is a section in the appendix.

\textit{I don't think it's really needed}

\end{document}
