\newcommand{\doc}{\ensuremath{z}}
\newcommand{\docs}{\ensuremath{\mathbf{z}}}
\newcommand{\answer}{\ensuremath{y}}
\newcommand{\genparam}{\ensuremath{\theta}}
\newcommand{\retparam}{\ensuremath{\eta}}
\newcommand{\query}{\ensuremath{x}}
\newcommand{\peranswer}{p_{\text{\tiny{RAG-Sequence}}}}
\newcommand{\pertoken}{p_{\text{\tiny{RAG-Token}}}}
\newcommand{\denc}{\mathbf{d}}
\newcommand{\qenc}{\mathbf{q}}
\newcommand{\topk}{\ensuremath{\text{top-k}(p_\eta(\cdot|\query))}}
\newcommand{\raganswer}{RAG-Sequence}
\newcommand{\ragtoken}{RAG-Token}
\newcommand{\history}[1]{{1:#1-1}}
% \newcommand{\history}[1]{{-#1}}


We explore RAG models, which use the input sequence \query{} to retrieve text documents \doc{} and use them as additional context when generating the target sequence \answer{}. As shown in Figure \ref{fig:fig_1}, our models leverage two components: (i) a retriever $p_\retparam(\doc|\query)$ with parameters \retparam{} that returns (top-K truncated) distributions over text passages given a query \query{} and (ii) a generator $p_\genparam(y_i|\query, \doc, y_\history{i})$ parametrized by \genparam{} that generates a current token based on a context of the previous $i-1$ tokens $y_\history{i}$, the original input \query{} and a retrieved passage \doc{}.  

To train the retriever and generator end-to-end, we treat the retrieved document as a latent variable. 
We propose two models that marginalize over the latent documents in different ways to produce a distribution over generated text. In one approach, \emph{\raganswer{}}, the model uses the same document to predict each target token. The second approach, \emph{\ragtoken{}}, can predict each target token based on a different document. In the following, we formally introduce both models and then describe the $p_\retparam$ and $p_\genparam$ components, as well as the training and decoding procedure.

\subsection{Models}
\paragraph{\raganswer{} Model}
The \raganswer{} model uses the same retrieved document to generate the complete \emph{sequence}. Technically, it treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability $p(\answer|\query)$ via a top-K approximation. Concretely, the top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized, 
\[
\peranswer(\answer|\query)\; \approx \; \sum_{\mathclap{\doc \in \text{top-}k(p(\cdot|\query))}} p_\retparam(\doc|\query) p_\genparam(\answer | \query, \doc)\; = \; \sum_{\mathclap{\doc \in \text{top-}k(p(\cdot|\query))}} p_\retparam(\doc|\query)\prod_i^N p_\genparam(y_i|\query, \doc, y_\history{i}) 
\]
\paragraph{\ragtoken{} Model}
In the \ragtoken{} model we can draw a different latent document for each target \emph{token} and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer. Concretely, the top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token,  Formally, we define:
\[
\pertoken(\answer|\query) \; \approx \; \prod_{i}^N \; \sum_{\doc \in \text{top-}k(p(\cdot|\query))} p_\retparam(\doc|\query) p_\genparam(y_i|\query, \doc, y_\history{i})
\]

Finally, we note that RAG can be used for sequence classification tasks by considering the target class as a target sequence of length one, in which case \raganswer{} and \ragtoken{} are equivalent.

\subsection{Retriever: DPR}
The retrieval component $p_\retparam(\doc|\query)$ is based on DPR~\cite{Karpukhin20dense}. DPR follows a bi-encoder architecture:
\[
p_\retparam(\doc|\query) \propto \exp \left(\denc(\doc)^{\top} \qenc(\query)\right) \; \; \; \; \; \; \; \; \; \denc(\doc) = \text{BERT}_{d}(z), \; \; \qenc(\query)=\text{BERT}_{q}(\query)
\] 
where $\denc(\doc)$ is a dense representation of a document produced by a BERT\textsubscript{BASE} \emph{document encoder}~\cite{devlin_bert:_2019}, and $\qenc(\query)$ a query representation produced by a \emph{query encoder}, also based on  BERT\textsubscript{BASE}. 
Calculating \topk{}, the list of $k$ documents \doc{} with highest prior probability $p_\eta(\doc|\query)$, is a Maximum Inner Product Search~(MIPS) problem, which can be approximately solved in sub-linear time~\cite{JDH17}. 
We use a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. This retriever was trained to retrieve documents which contain answers to  TriviaQA~\cite{joshi_triviaqa:_2017} questions and Natural Questions~\cite{kwiatkowski_natural_2019}. We refer to the document index as the \emph{non-parametric memory}.

\subsection{Generator: BART}
The generator component $p_\genparam(y_i|\query, \doc, y_\history{i})$ could be modelled using any encoder-decoder. We use BART-large~\cite{lewis2019bart}, a pre-trained seq2seq transformer~\cite{vaswani2017attention} with 400M parameters. To combine the input \query{} with the retrieved content \doc{} when generating from BART, we simply concatenate them.
BART was pre-trained using a denoising objective and a variety of different noising functions. It has obtained state-of-the-art results on a diverse set of generation tasks and outperforms comparably-sized T5 models~\cite{lewis2019bart}. 
We refer to the BART generator parameters \genparam{} as the \emph{parametric memory} henceforth.


\subsection{Training}
We jointly train the retriever and generator components without any direct supervision on what document should be retrieved.
Given a fine-tuning training corpus of input/output pairs $(\query_j,\answer_j)$, we minimize the negative marginal log-likelihood of each target, $\sum_j -\log p(\answer_j|\query_j)$ using stochastic gradient descent with Adam~\cite{kingma_adam}.
Updating the document encoder $\text{BERT}_d$ during training is costly as it requires the document index to be periodically updated as REALM does during pre-training~\cite{guu2020realm}. We do not find this step necessary for strong performance, and keep the document encoder (and index) fixed, only fine-tuning the query encoder $\text{BERT}_q$ and the BART generator.

\subsection{Decoding}

At test time, \raganswer{} and \ragtoken{} require different ways to approximate $\argmax_\answer p(\answer|\query)$.

\paragraph{\ragtoken{}} The \ragtoken{} model can be seen as a standard, autoregressive seq2seq generator with transition probability: 
$p'_\genparam(y_i|\query, y_\history{i}) = \sum_{\doc \in \text{top-}k(p(\cdot|\query))} p_\retparam(\doc_i|\query) p_\genparam(y_i|\query, \doc_i, y_\history{i})$
To decode, we can plug $p'_\genparam(y_i|\query, y_\history{i})$ into a standard beam decoder.

\paragraph{\raganswer{}} 
For \raganswer{}, the likelihood $p(\answer|\query)$ does not break into a conventional per- token likelihood, hence we cannot solve it with a single beam search. Instead, we run beam search for each  document $\doc$, scoring each hypothesis using $p_\genparam(y_i|\query, \doc, y_\history{i})$. This yields a set of hypotheses $Y$, some of which may not have appeared in the beams of all documents. To estimate the probability of an hypothesis $\answer$ 
we run an additional forward pass for each document \doc{} for which $\answer$ does not appear in the beam, multiply generator probability with $p_\retparam(\doc|\query)$ and then sum the probabilities across beams for the marginals. We refer to this decoding procedure as ``Thorough Decoding.''
For longer output sequences, $|Y|$ can become large, requiring many forward passes. For more efficient decoding, we can make a further approximation that $p_\genparam(y|\query, \doc_i) \approx 0$ where $y$ was not generated during beam search from $\query, \doc_i$. This avoids the need to run additional forward passes once the candidate set $Y$ has been generated. We refer to this decoding procedure as ``Fast Decoding.''
