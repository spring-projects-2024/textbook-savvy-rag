\documentclass{article}



% if you need to pass options to natbib, use, e.g.:
    \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2020

% ready for submission
% \usepackage{neurips_2020}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
\usepackage[preprint]{neurips_2020}
\usepackage{subfig}
\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{arg\,max}


% to compile a camera-ready version, add the [final] option, e.g.:
    % \usepackage[final]{neurips_2020}

% to avoid loading the natbib package, add option nonatbib:
    %  \usepackage[nonatbib]{neurips_2020}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{color}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{soul}
\usepackage{array}
\usepackage{wrapfig}
\usepackage{mathtools}
\colorlet{soulgray}{lightgray!30}
\sethlcolor{soulgray}
\input{comment_engine.tex}

%\title{Retrieval-Augmented Generation (Title needed)}
%\title{Retrieval-Augmented Generation for NLP} % is this general enough? ;p
%\title{Open-Domain Retrieval-Augmented Generation}
\title{Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  Patrick Lewis${}^\dagger{}^\ddagger$, Ethan Perez$^\star$,\And
  Aleksandra Piktus$^\dagger$, Fabio Petroni$^\dagger$, Vladimir Karpukhin$^\dagger$, Naman Goyal$^\dagger$, Heinrich K\"uttler$^\dagger$,\And
  Mike Lewis$^\dagger$, Wen-tau Yih$^\dagger$, Tim Rockt\"aschel${}^\dagger{}^\ddagger$, Sebastian Riedel${}^\dagger{}^\ddagger$, Douwe Kiela$^\dagger$\vspace{15pt}\\
  %\thanks{Use footnote for providing further information about author (webpage, alternative address)---\emph{not} for acknowledging funding agencies.} \\
  $^\dagger$Facebook AI Research; $^\ddagger$University College London; $^\star$New York University;\vspace{2pt}\\
  \texttt{plewis@fb.com}\\
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}

 
\begin{document}

\maketitle

\begin{abstract}
\input{abstract.tex}
\end{abstract}

\section{Introduction}
\input{intro.tex}

\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{images/RAG-Architecture.pdf}
\caption{Overview of our approach. We combine a pre-trained retriever~(\emph{Query Encoder} + \emph{Document Index}) with a pre-trained seq2seq model (\emph{Generator}) and fine-tune end-to-end. For  query $x$, we use Maximum Inner Product Search (MIPS) to find the top-K documents $z_i$. For final prediction $y$, we treat $z$ as a latent variable and marginalize over seq2seq predictions given different documents.}
\label{fig:fig_1}
\end{figure}

\section{Methods}
\input{methods.tex}

\section{Experiments}
\input{experiments.tex}

\section{Results}
\input{results.tex}

\section{Related Work}
\input{related.tex}

\section{Discussion}

% Recent advances in pre-trained neural language models have shown that such models not only learn powerful representations, but that they are also capable of storing a substantial amount of in-depth knowledge in their parameters.
% % However, they require huge amounts of parameters to be successful, are cumbersome to train and their parametric memory cannot easily be revised or expanded.
% However, they can struggle to accurately access and manipulate this knowledge, as sell as 
In this work, we presented hybrid generation models with access to parametric and non-parametric memory.
We showed that our RAG models obtain state of the art results on open-domain QA. We found that people prefer RAG's generation over purely parametric BART, finding RAG more factual and specific.
We conducted an thorough investigation of the learned retrieval component, validating its effectiveness, and we illustrated how the retrieval index can be hot-swapped to update the model without requiring any retraining.
In future work, it may be fruitful to investigate if the two components can be jointly pre-trained from scratch, either with a denoising objective similar to BART or some another objective. 
Our work opens up new research directions on how parametric and non-parametric memories interact and how to most effectively combine them, showing promise in being applied to a wide variety of NLP tasks.

% What goes here? The promise of pretraining end-to-end? The problem of posterior collapse?

% \TODO{why not wizard of wikipedia?}
% \douwe{because that's just learning tf-idf, right?}
% \patrick{yep, but we might want to state that somewhere}

% \ethan{Should we discuss posterior collapse here?}

% \TODO{discuss numbers of parameters}
% \TODO{make interpretability arguments explicit}
% \TODO{ensure we qualitatively discuss generations}
% \patrick{multihop things?}
% \patrick{mention the null doc things?}


%%% NOTE: FINALIZE AND REINSERT FOR NEURIPS SUBMISSION -- not included in arxiv submission
\clearpage 
% Patrick: broader impacts doestn count towards 8 page limit
\section*{Broader Impact}

%% Optimistic ending version
% \ethan{
% Since RAG can be employed as a language model, similar concerns for GPT-2~\cite{radford2019language} are valid here, although arguably to a lesser extent, including that it might be used to generate abuse, faked or misleading content in the news or on social media; to impersonate others; or to automate the production of spam/phishing content~\cite{solaiman2019release}.
% Our retrieval-based approach may make RAG more likely to generate factual knowledge, but Wikipedia, or any potential external knowledge source, will likely never be entirely factual and completely devoid of bias. Advanced language models may also lead to the automation of various jobs in the coming decades~\cite{grace2017when}.}

% \ethan{These potential downsides are also accompanied by several advantages.
% This work offers a few positive societal benefits over previous work: the grounded in real, factual knowledge (in this case Wikipedia) makes RAG ``hallucinate'' less, with generations that are more factual, and offers more control (over which knowledge sources are used) and interpretability (retrieved documents are human-readable). RAG could be employed in a wide variety of scenarios with direct benefit to society, for example by endowing it with a medical index and asking it open-domain questions on that topic, or by helping people be more effective at their jobs by answering questions or checking facts against trusted sources.
% }

This work offers several positive societal benefits over previous work: the fact that it is more strongly grounded in real factual knowledge (in this case Wikipedia) makes it ``hallucinate'' less with generations that are more factual, and offers more control and interpretability. RAG could be employed in a wide variety of scenarios with direct benefit to society, for example by endowing it with a medical index and asking it open-domain questions on that topic, or by helping people be more effective at their jobs.

With these advantages also come potential downsides: Wikipedia, or any potential external knowledge source, will probably never be entirely factual and completely devoid of bias. Since RAG can be employed as a language model, similar concerns as for GPT-2 \cite{radford2019language} are valid here, although arguably to a lesser extent, including that it might be used to generate abuse, faked or misleading content in the news or on social media; to impersonate others; or to automate the production of spam/phishing content~\cite{solaiman2019release}. Advanced language models may also lead to the automation of various jobs in the coming decades~\cite{grace2017when}.
In order to mitigate these risks, AI systems could be employed to fight against misleading content and automated spam/phishing.

\section*{Acknowledgments}
The authors would like to thank the reviewers for their thoughtful and constructive feedback on this paper, as well as HuggingFace for their help in open-sourcing code to run RAG models.
The authors would also like to thank Kyunghyun Cho and Sewon Min for productive discussions and advice.
EP thanks supports from the NSF Graduate Research Fellowship. PL is supported by the FAIR PhD program.


\bibliographystyle{plainnat}
\bibliography{acl_full_anthology,references}

\clearpage

\appendix
\input{appendix.tex}

\end{document}