
\paragraph{Single-Task Retrieval}
Prior work has shown that retrieval improves performance across a variety of NLP tasks when considered in isolation.
Such tasks include open-domain question answering~\cite{chen_reading_2017,kwiatkowski_natural_2019},  fact checking~\cite{thorne-etal-2018-fever}, fact completion~\cite{petroni2020how}, long-form question answering~\cite{fan-etal-2019-eli5}, Wikipedia article generation~\cite{liu2018generating}, dialogue~\cite{moghe-etal-2018-towards,weston-etal-2018-retrieve,dinan2018wizard,fan2020augmenting}, translation~\cite{gu2018search}, and language modeling~\cite{guu-etal-2018-generating,khandelwal2020generalization}.
Our work unifies previous successes in incorporating retrieval into individual tasks, showing that a single retrieval-based architecture is capable of achieving strong performance across several tasks.

\paragraph{General-Purpose Architectures for NLP}
Prior work on general-purpose architectures for NLP tasks has shown great success without the use of retrieval.
A single, pre-trained language model has been shown to achieve strong performance on various classification tasks in the GLUE benchmarks~\cite{wang-etal-2018-glue,wang_superglue_2019} after fine-tuning~\cite{radford_improving_2018,devlin_bert:_2019}.
GPT-2~\citep{radford2019language} later showed that a single, left-to-right, pre-trained language model could achieve strong performance across both discriminative and generative tasks.
For further improvement, BART~\cite{lewis2019bart} and T5~\cite{raffel2019t5,roberts2020t5cqba} propose a single, pre-trained encoder-decoder model that leverages bi-directional attention to achieve stronger performance on discriminative and generative tasks.
Our work aims to expand the space of possible tasks with a single, unified architecture, by learning a retrieval module to augment pre-trained, generative language models.

\paragraph{Learned Retrieval}
There is significant work on learning to retrieve documents in information retrieval, more recently with pre-trained, neural language models~\cite{nogueira2019passage,Karpukhin20dense} similar to ours.
Some work optimizes the retrieval module to aid in a specific, downstream task such as question answering, using search~\cite{perez-etal-2019-finding}, reinforcement learning~\cite{choi-etal-2017-coarse,wang2018evidence-aggregation,wang2018r3}, or a latent variable approach~\cite{lee-etal-2019-latent,guu2020realm} as in our work.
These successes leverage different retrieval-based architectures and optimization techniques to achieve strong performance on a single task, while we show that a single retrieval-based architecture can be fine-tuned for strong performance on a variety of tasks.

\paragraph{Memory-based Architectures}
Our document index can be seen as a large external memory for neural networks to attend to, analogous to memory networks~\cite{weston2015memory,sukhbaatar2015end}.
Concurrent work~\cite{fevry2020entities} learns to retrieve a trained embedding for each entity in the input, rather than to retrieve raw text as in our work.
Other work improves the ability of dialog models to generate factual text by attending over fact embeddings~\cite{ghazvininejad2018knowledge,fan2020augmenting}. A key feature of our memory is that it is comprised of raw text rather distributed representations, which makes the memory both (i) human-readable, lending a form of interpretability to our model, and (ii) human-writable, enabling us to dynamically update the model's memory by editing the document index. This approach has also been used in knowledge-intensive dialog, where generators have been conditioned on retrieved text directly, albeit obtained via TF-IDF rather than end-to-end learnt retrieval~\cite{dinan2018wizard}.


\paragraph{Retrieve-and-Edit approaches} Our method shares some similarities with retrieve-and-edit style approaches, where a similar training input-output pair is retrieved for a given input, and then edited to provide a final output. These approaches have proved successful in a number of domains including  Machine Translation~ \cite{gu_mt_retrieve_and_edit,hossain-etal-2020-simple} and Semantic Parsing~\cite{NIPS2018_8209}. Our approach does have several differences, including less of emphasis on lightly editing a retrieved item, but on aggregating content from several pieces of retrieved content, as well as learning latent retrieval, and retrieving evidence documents rather than related training pairs. This said, RAG techniques may work well in these settings, and could represent promising future work.


