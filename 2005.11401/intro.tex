Pre-trained neural language models have been shown to learn a substantial amount of in-depth knowledge from data~\cite{petroni-etal-2019-language}. 
They can do so without any access to an external memory, as a parameterized implicit knowledge base~\cite{raffel2019t5,roberts2020t5cqba}. 
While this development is exciting, such models do have downsides: 
They cannot easily expand or revise their memory, can't straightforwardly provide insight into their predictions, and may produce ``hallucinations''~\cite{marcus2020next}.
Hybrid models that combine parametric memory with non-parametric (i.e., retrieval-based) memories~\cite{guu2020realm,Karpukhin20dense,petroni2020how} can address some of these issues because knowledge 
can be directly revised and expanded, and accessed knowledge can be inspected and interpreted. 
REALM~\cite{guu2020realm} and ORQA~\cite{lee-etal-2019-latent}, two recently introduced models that combine masked language models~\cite{devlin_bert:_2019} with a differentiable retriever, have shown promising results, but have only explored open-domain extractive question answering. 
Here, we bring hybrid parametric and non-parametric memory to the ``workhorse of NLP,'' i.e. sequence-to-sequence~(seq2seq) models.

We endow pre-trained, parametric-memory generation models with a non-parametric memory through a general-purpose fine-tuning approach which we refer to as retrieval-augmented generation~(RAG).
We build RAG models where the parametric memory is a pre-trained seq2seq transformer, and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We combine these components in a probabilistic model trained end-to-end (Fig. \ref{fig:fig_1}). The retriever~(Dense Passage Retriever~\cite{Karpukhin20dense}, henceforth DPR) provides latent documents conditioned on the input, and the seq2seq model~(BART~\cite{lewis2019bart}) then conditions on these latent documents together with the input to generate the output. 
We marginalize the latent documents with a top-K approximation, either on a per-output basis~(assuming the same document is responsible for all tokens) or a per-token basis~(where different documents are responsible for different tokens). Like T5~\cite{raffel2019t5} or BART, RAG can be fine-tuned on any seq2seq task, whereby both the generator and retriever are jointly learned. 

There has been extensive previous work proposing architectures to enrich systems with non-parametric memory which are trained from scratch for specific tasks, e.g. memory networks~\cite{weston2015memory,sukhbaatar2015end}, stack-augmented networks~\cite{joulin2015} and memory layers~\cite{lample_large_2019}. In contrast, we explore a setting where both parametric and non-parametric memory components are pre-trained and pre-loaded with extensive knowledge. Crucially, by using pre-trained access mechanisms, the ability to access knowledge is present without additional training. 

Our results highlight the benefits of combining  parametric and non-parametric memory with generation for \emph{knowledge-intensive tasks}---tasks that humans could not reasonably be expected to perform without access to an external knowledge source. 
Our RAG models achieve state-of-the-art results on open Natural Questions~\cite{kwiatkowski_natural_2019}, WebQuestions~\cite{berant_semantic_2013} and CuratedTrec~\cite{baudivs2015modeling} and strongly outperform recent approaches that use specialised pre-training objectives on TriviaQA~\cite{joshi_triviaqa:_2017}. Despite these being extractive tasks, we find that unconstrained generation outperforms previous extractive approaches. For knowledge-intensive generation, we experiment with MS-MARCO~\cite{bajaj_ms_2016} and Jeopardy question generation, and we find that our models generate responses that are more factual, specific, and diverse than a BART baseline. For FEVER~\cite{thorne-etal-2018-fever} fact verification, we achieve results within 4.3\% of state-of-the-art pipeline models which use strong retrieval supervision.
Finally, we demonstrate that the non-parametric memory can be replaced
to update the models' knowledge as the world changes.\footnote{Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transformers Library~\cite{Wolf2019HuggingFacesTS} and can be found at \url{https://github.com/huggingface/transformers/blob/master/examples/rag/}. An interactive demo of RAG models can be found at \url{https://huggingface.co/rag/}}

